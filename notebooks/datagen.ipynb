{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU:  cuda:0\n",
      "Running on GPU:  cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "# import line_profiler as lp \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, functional\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "# from model import *\n",
    "from model2D import *\n",
    "from datagen import *\n",
    "from analysis import *\n",
    "from datagen2D import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on GPU: \", device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "torch.set_default_device(device)\n",
    "# torch.cuda.synchronize()\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 193/500 [00:01<00:03, 97.59it/s] [2023-08-12 23:58:17,559] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:17,808] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 41%|████      | 203/500 [00:02<00:07, 37.66it/s][2023-08-12 23:58:18,241] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:18,485] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:18,700] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:18,946] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 42%|████▏     | 211/500 [00:03<00:13, 21.51it/s][2023-08-12 23:58:19,171] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:19,411] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 43%|████▎     | 217/500 [00:03<00:15, 18.82it/s][2023-08-12 23:58:19,639] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:19,878] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 44%|████▍     | 222/500 [00:04<00:16, 16.49it/s][2023-08-12 23:58:20,109] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:20,351] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 45%|████▌     | 226/500 [00:04<00:19, 14.20it/s][2023-08-12 23:58:20,580] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:20,850] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 46%|████▌     | 231/500 [00:05<00:21, 12.68it/s][2023-08-12 23:58:21,118] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:21,379] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 47%|████▋     | 236/500 [00:05<00:22, 11.78it/s][2023-08-12 23:58:21,611] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:21,863] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 48%|████▊     | 241/500 [00:06<00:22, 11.38it/s][2023-08-12 23:58:22,101] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:22,340] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 49%|████▉     | 246/500 [00:06<00:23, 11.02it/s][2023-08-12 23:58:22,588] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:22,831] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 50%|█████     | 251/500 [00:07<00:23, 10.80it/s][2023-08-12 23:58:23,064] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:23,302] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 51%|█████     | 256/500 [00:07<00:22, 10.82it/s][2023-08-12 23:58:23,531] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:23,767] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 52%|█████▏    | 261/500 [00:07<00:22, 10.79it/s][2023-08-12 23:58:23,997] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:24,239] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 53%|█████▎    | 266/500 [00:08<00:21, 10.79it/s][2023-08-12 23:58:24,466] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:24,705] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 54%|█████▍    | 271/500 [00:08<00:21, 10.73it/s][2023-08-12 23:58:24,929] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:25,172] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 55%|█████▌    | 276/500 [00:09<00:20, 10.74it/s][2023-08-12 23:58:25,424] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:25,669] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 56%|█████▌    | 281/500 [00:10<00:23,  9.19it/s][2023-08-12 23:58:26,125] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:26,375] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 57%|█████▋    | 286/500 [00:10<00:22,  9.56it/s][2023-08-12 23:58:26,611] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:26,853] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 58%|█████▊    | 291/500 [00:11<00:21,  9.81it/s][2023-08-12 23:58:27,094] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:27,340] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 59%|█████▉    | 296/500 [00:11<00:20,  9.93it/s][2023-08-12 23:58:27,588] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:27,835] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 60%|██████    | 301/500 [00:12<00:19,  9.96it/s][2023-08-12 23:58:28,087] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:28,341] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 61%|██████    | 306/500 [00:12<00:19,  9.96it/s][2023-08-12 23:58:28,573] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2023-08-12 23:58:28,815] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      " 62%|██████▏   | 311/500 [00:13<00:18, 10.12it/s][2023-08-12 23:58:29,041] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)\n",
      "   function: 'smooth_wandering_2D_squarefix_randomstart_hdv' (/media/mattis/761E73AB1E7362D3/Users/kingt/Dropbox/Jobb/BioAI/RNN-Adding-Problem-Representations/notebooks/../src/datagen2D.py:181)\n",
      "   reasons:  t_steps == 200\n",
      "to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.\n",
      "100%|██████████| 500/500 [00:16<00:00, 31.15it/s]\n",
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/numpy/lib/npyio.py:528: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.asanyarray(arr)\n",
      "/home/mattis/anaconda3/envs/torch2/lib/python3.10/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.asanyarray(arr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last training time steps: 101\n"
     ]
    }
   ],
   "source": [
    "# # Generate dataset and save in ../datasets\n",
    "# # First generate gradual gradual dataset\n",
    "\n",
    "def train_gradual_gen(epochs,batch_size=64,per=15):\n",
    "    i = 0\n",
    "    training_steps = 1\n",
    "    gen_data = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # data,labels = smooth_wandering_2D_squarefix(batch_size,training_steps,bound=0.5,v_sigma=0.01,d_sigma=0.08,v_bound_reduction=0.15,stability=0.0)\n",
    "        data,labels = smooth_wandering_2D_squarefix_randomstart_hdv(batch_size,training_steps,bound=0.5,v_sigma=0.01,d_sigma=0.08,v_bound_reduction=0.15,stability=0.0)\n",
    "        # data,labels = smooth_wandering_2D_squarefix(test_batch_size,t_test)\n",
    "        # data,labels = rat_box(batch_size,training_steps)\n",
    "        gen_data.append((data.cpu().detach(),labels.cpu().detach()))\n",
    "        i+=1\n",
    "        if i%per == 0:\n",
    "            training_steps += 1\n",
    "    print(\"Last training time steps:\",training_steps)\n",
    "    return gen_data\n",
    "    # Save dataset\n",
    "\n",
    "def train_constant(epochs,batch_size=64,training_steps=200):\n",
    "    gen_data = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # data,labels = smooth_wandering_2D_squarefix(batch_size,training_steps,bound=0.5,v_sigma=0.01,d_sigma=0.1,v_bound_reduction=0.15,stability=0.01)\n",
    "        data,labels = smooth_wandering_2D_squarefix_randomstart_hdv(batch_size,training_steps,bound=0.5,v_sigma=0.01,d_sigma=0.08,v_bound_reduction=0.15,stability=0.0)\n",
    "        # data,labels = smooth_wandering_2D_squarefix(test_batch_size,t_test)\n",
    "        # data,labels = rat_box(batch_size,training_steps)\n",
    "        gen_data.append((data.cpu().detach(),labels.cpu().detach()))\n",
    "    print(\"Last training time steps:\",training_steps)\n",
    "    return gen_data\n",
    "\n",
    "batch_size = 64\n",
    "per = 5\n",
    "epochs = 500\n",
    "t_steps = 200\n",
    "gen_data = train_gradual_gen(epochs,batch_size=batch_size,per=per)\n",
    "# gen_data = train_constant(epochs,batch_size=batch_size,training_steps=t_steps)\n",
    "np.save(\"../datasets/gradual_2D_hdv_randomstart_{}_{}_{}\".format(epochs,per,batch_size),gen_data) # Will be saved as [Epochs,data/labels,batchsize,tsteps,x/y]\n",
    "# np.save(\"../datasets/constant_2D_hdv_randomstart_{}_{}_{}\".format(epochs,t_steps,batch_size),gen_data) # Will be saved as [Epochs,data/labels,batchsize,tsteps,x/y]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datamodel(Dataset):\n",
    "    def __init__(self,time_pos_points,labels):\n",
    "        self.x = time_pos_points\n",
    "        self.y = labels\n",
    "    def __len__(self): \n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, ix):\n",
    "        return self.x[ix], self.y[ix]\n",
    "    \n",
    "data = input_data[:,0]\n",
    "labels = input_data[:,1]\n",
    "datamodel = Datamodel(data, labels)\n",
    "data_loader = DataLoader(datamodel, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = input_data[4999,1][:,:,:].squeeze()\n",
    "# path.shape\n",
    "\n",
    "# n = 24\n",
    "# plt.plot(path[n,:,0], path[n,:,1], '--o',alpha=0.5)\n",
    "# # # Plot first\n",
    "# plt.plot(path[n,0,0],path[n,0,1],\"o\",color=\"green\")\n",
    "# # # Plot last\n",
    "# plt.plot(path[n,-1,0],path[n,-1,1],\"o\",color=\"orange\")\n",
    "# plt.title(\"2D path example\")\n",
    "# plt.xlim(-np.pi,np.pi)\n",
    "# plt.ylim(-np.pi,np.pi)\n",
    "# plt.xlabel(\"$x$\")\n",
    "# plt.ylabel(\"$y$\")\n",
    "# plt.legend([\"Path\",\"Start\",\"End\"])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
